{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d53b07c",
   "metadata": {},
   "source": [
    "Context\n",
    "\n",
    "Spam is unsolicited and unwanted messages sent electronically and whose content may be malicious. \n",
    "Email spam is sent/received over the Internet while SMS spam is typically transmitted over a mobile network. \n",
    "We’ll refer to user that sent spam as ‘spammers’. SMS messages are usually very cheap (if not free) for the user to send, \n",
    "making it appealing for unrightful exploitation. This is further aggravated by the fact that SMS is usually regarded \n",
    "by the user as a safer, more trustworthy form of communication than other sources, e. g., emails.\n",
    "\n",
    "The dangers of spam messages for the users are many: undesired advertisement, exposure of private information, \n",
    "becoming a victim of a fraud or financial scheme, being lured into malware and phishing websites, involuntary exposition \n",
    "to inappropriate content, etc. For the network operator, spam messages result in an increased cost in operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd22c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the libraries to be used\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline    \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import precision_score, recall_score, plot_confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\vivih\\Downloads\\spam.csv\", encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10fd16f",
   "metadata": {},
   "source": [
    "this dataset has three Unnamed columns that we don't need, so we just drop them\n",
    "also our label is in string form -> spam and ham, so we map them in numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe975a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping the redundent looking collumns (for this project)\n",
    "to_drop = [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"]\n",
    "df = df.drop(df[to_drop], axis=1)\n",
    "# Renaming the columns because I feel fancy today \n",
    "df.rename(columns = {\"v1\":\"Target\", \"v2\":\"Text\"}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d55884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfac681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd158a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a78e62",
   "metadata": {},
   "source": [
    "The dataset consists of 5,574 messages in English. The data is designated as being ham or spam. Dataframe has two columns. \n",
    "The first column is \"Target\" indicating the class of message as ham or spam and the second \"Text\" column is the string of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e5e14",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= [\"#32cd32\", \"#1e90ff\"] \n",
    "#first of all let us evaluate the target and find out if our data is imbalanced or not\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= df[\"Target\"], palette= cols)\n",
    "fg.set_title(\"Count Plot of Classes\", color=\"#000000\")\n",
    "fg.set_xlabel(\"Classes\", color=\"#000000\")\n",
    "fg.set_ylabel(\"Number of Data points\", color=\"#000000\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d27be",
   "metadata": {},
   "source": [
    "For the purpose of data exploration, i am creating new features\n",
    "\n",
    "Sum_characters: Number of characters in the text message\n",
    "Sum_words: Number of words in the text message\n",
    "Sum_sentence: Number of sentences in the text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sum_characters\"] = df[\"Text\"].apply(len)\n",
    "df[\"Sum__words\"]=df.apply(lambda row: nltk.word_tokenize(row[\"Text\"]), axis=1).apply(len)\n",
    "df[\"Sum_sentence\"]=df.apply(lambda row: nltk.sent_tokenize(row[\"Text\"]), axis=1).apply(len)\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3607961",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data=df, hue=\"Target\",palette=cols)\n",
    "plt.show(fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9934ca",
   "metadata": {},
   "source": [
    "Note: From the pair plot, we can see a few outliers all in the class ham. This is interesting as \n",
    "we could put a cap over one of these. As they essentially indicate the same thing ie the length of SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69277ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outliers. \n",
    "df = df[(df[\"Sum_characters\"]<350)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data=df, hue=\"Target\",palette=cols)\n",
    "plt.show(fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f72e9b",
   "metadata": {},
   "source": [
    "Data cleaning is a very crucial step in any machine learning model, but more so for NLP. \n",
    "Without the cleaning process, the dataset is often a cluster of words that the computer doesn’t understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to clean up the text\n",
    "def Clean(Text):\n",
    "    sms = re.sub('[^a-zA-Z]', ' ', Text) \n",
    "    sms = sms.lower()\n",
    "    sms = sms.split()\n",
    "    sms = ' '.join(sms)\n",
    "    return sms\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(Clean)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after cleaning:\\033[0m\",*df[\"Clean_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396058f",
   "metadata": {},
   "source": [
    "Tokenization is the first step in any NLP pipeline. It has an important effect on the rest of your pipeline. \n",
    "A tokenizer breaks unstructured data and natural language text into chunks of information that can be \n",
    "considered as discrete elements. The token occurrences in a document can be used directly \n",
    "as a vector representing that document. \n",
    "\n",
    "This immediately turns an unstructured string (text document) into \n",
    "a numerical data structure suitable for machine learning. They can also be used directly by a computer \n",
    "to trigger useful actions and responses. Or they might be used in a machine learning pipeline \n",
    "as features that trigger more complex decisions or behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tokenize_Text\"]=df.apply(lambda row: nltk.word_tokenize(row[\"Clean_Text\"]), \n",
    "                                 axis=1)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after Tokenizing:\\033[0m\",\n",
    "      *df[\"Tokenize_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cb828",
   "metadata": {},
   "source": [
    "The process of converting data to something a computer can understand \n",
    "is referred to as pre-processing. One of the major forms of pre-processing \n",
    "is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "df[\"Nostopword_Text\"] = df[\"Tokenize_Text\"].apply(remove_stopwords)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after removing the stopwords:\\033[0m\",*df[\"Nostopword_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15969646",
   "metadata": {},
   "source": [
    "Stemming is a natural language processing technique that lowers inflection in words \n",
    "to their root forms, hence aiding in the preprocessing of text, words, and documents for text normalization.\n",
    "\n",
    "According to Wikipedia, inflection is the process through which a word is \n",
    "modified to communicate many grammatical categories, including tense, case, \n",
    "voice, aspect, person, number, gender, and mood. Thus, although a word may exist in several inflected forms, \n",
    "having multiple inflected forms inside the same text adds redundancy to the NLP process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d703df6",
   "metadata": {},
   "source": [
    "Lemmatization entails reducing a word to its canonical or dictionary form. \n",
    "The root word is called a ‘lemma’.The method entails assembling the inflected parts \n",
    "of a word in a way that can be recognised as a single element. \n",
    "The process is similar to stemming but the root words have meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09deebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "    #word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas\n",
    "\n",
    "df[\"Lemmatized_Text\"] = df[\"Nostopword_Text\"].apply(lemmatize_word)\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after lemitization:\\033[0m\",*df[\"Lemmatized_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03968170",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates \n",
    "how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, \n",
    "and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "It has many uses, most importantly in automated text analysis, and is very useful for scoring words \n",
    "in machine learning algorithms for Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= []\n",
    "for i in df[\"Lemmatized_Text\"]:\n",
    "    msg = ' '.join([row for row in i])\n",
    "    corpus.append(msg)\n",
    "    \n",
    "corpus[:5]\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 lines in corpus :\\033[0m\",*corpus[:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57259617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing text data in to numbers. \n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "#Let's have a look at our feature \n",
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"Target\"] = label_encoder.fit_transform(df[\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68fc9a",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting values for labels and feature as y and X(we already did X in vectorizing...)\n",
    "y = df[\"Target\"] \n",
    "# Splitting the testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on the following classifiers\n",
    "classifiers = [MultinomialNB(), \n",
    "               RandomForestClassifier(),\n",
    "               KNeighborsClassifier(), \n",
    "               SVC()]\n",
    "for cls in classifiers:\n",
    "    cls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff55bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of pipelines and model types for ease of reference\n",
    "pipe_dict = {0: \"NaiveBayes\", 1: \"RandomForest\", 2: \"KNeighbours\",3: \"SVC\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes: 0.962856 \n"
     ]
    }
   ],
   "source": [
    "# Cossvalidation \n",
    "for i, model in enumerate(classifiers):\n",
    "    cv_score = cross_val_score(model, X_train,y_train,scoring=\"accuracy\", cv=10)\n",
    "    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561de1c5",
   "metadata": {},
   "source": [
    "Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision =[]\n",
    "recall =[]\n",
    "f1_score = []\n",
    "trainset_accuracy = []\n",
    "testset_accuracy = []\n",
    "\n",
    "\n",
    "for i in classifiers:\n",
    "    pred_train = i.predict(X_train)\n",
    "    pred_test = i.predict(X_test)\n",
    "    prec = metrics.precision_score(y_test, pred_test)\n",
    "    recal = metrics.recall_score(y_test, pred_test)\n",
    "    f1_s = metrics.f1_score(y_test, pred_test)\n",
    "    train_accuracy = model.score(X_train,y_train)\n",
    "    test_accuracy = model.score(X_test,y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31456efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data of lists.\n",
    "data = {'Precision':precision,\n",
    "'Recall':recall,\n",
    "'F1score':f1_score,\n",
    "'Accuracy on Testset':testset_accuracy,\n",
    "'Accuracy on Trainset':trainset_accuracy}\n",
    "# Creates pandas DataFrame.\n",
    "Results = pd.DataFrame(data, index =[\"NaiveBayes\", \"RandomForest\", \"KNeighbours\",\"SVC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap2 = ListedColormap([\"#32cd32\", \"#1e90ff\"])\n",
    "Results.style.background_gradient(cmap=cmap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da95bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
